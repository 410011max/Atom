{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "smooth_scales = torch.load(\"act_scales/llama2-7b-hf-smooth.pt\")\n",
    "act_scales = torch.load(\"act_scales/llama2-7b-hf-static.pt\")\n",
    "act_w_scales = torch.load(\"act_scales/llama2-7b-hf-static-weight.pt\")\n",
    "smooth_act_scales = torch.load(\"act_scales/llama2-7b-hf-smooth-static.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj 0.7734375 0.02099609375\n",
      "model.layers.0.self_attn.k_proj 0.81640625 0.0224609375\n",
      "model.layers.0.self_attn.v_proj 0.134765625 0.0223388671875\n",
      "model.layers.0.self_attn.o_proj 0.451171875 0.0281982421875\n",
      "model.layers.0.mlp.gate_proj 0.890625 0.0380859375\n",
      "model.layers.0.mlp.up_proj 0.341796875 0.038818359375\n",
      "model.layers.0.mlp.down_proj 0.52734375 0.055908203125\n",
      "model.layers.1.self_attn.q_proj 0.47265625 0.0242919921875\n",
      "model.layers.1.self_attn.k_proj 0.57421875 0.0302734375\n",
      "model.layers.1.self_attn.v_proj 0.1279296875 0.0147705078125\n",
      "model.layers.1.self_attn.o_proj 0.58984375 0.031494140625\n",
      "model.layers.1.mlp.gate_proj 1.09375 0.052978515625\n",
      "model.layers.1.mlp.up_proj 0.40625 0.04248046875\n",
      "model.layers.1.mlp.down_proj 1.5625 0.05859375\n",
      "model.layers.2.self_attn.q_proj 1.1015625 0.0400390625\n",
      "model.layers.2.self_attn.k_proj 0.373046875 0.0277099609375\n",
      "model.layers.2.self_attn.v_proj 0.1474609375 0.031494140625\n",
      "model.layers.2.self_attn.o_proj 0.60546875 0.043212890625\n",
      "model.layers.2.mlp.gate_proj 0.52734375 0.056640625\n",
      "model.layers.2.mlp.up_proj 0.455078125 0.050048828125\n",
      "model.layers.2.mlp.down_proj 0.8984375 0.06005859375\n",
      "model.layers.3.self_attn.q_proj 0.78125 0.0279541015625\n",
      "model.layers.3.self_attn.k_proj 0.41015625 0.030517578125\n",
      "model.layers.3.self_attn.v_proj 0.138671875 0.0203857421875\n",
      "model.layers.3.self_attn.o_proj 0.5390625 0.041259765625\n",
      "model.layers.3.mlp.gate_proj 0.578125 0.05029296875\n",
      "model.layers.3.mlp.up_proj 0.50390625 0.048828125\n",
      "model.layers.3.mlp.down_proj 0.68359375 0.059814453125\n",
      "model.layers.4.self_attn.q_proj 0.67578125 0.041748046875\n",
      "model.layers.4.self_attn.k_proj 0.51171875 0.03759765625\n",
      "model.layers.4.self_attn.v_proj 0.142578125 0.035400390625\n",
      "model.layers.4.self_attn.o_proj 0.62109375 0.042236328125\n",
      "model.layers.4.mlp.gate_proj 0.51953125 0.0517578125\n",
      "model.layers.4.mlp.up_proj 0.478515625 0.046630859375\n",
      "model.layers.4.mlp.down_proj 0.9765625 0.057373046875\n",
      "model.layers.5.self_attn.q_proj 0.640625 0.041015625\n",
      "model.layers.5.self_attn.k_proj 0.333984375 0.03515625\n",
      "model.layers.5.self_attn.v_proj 0.265625 0.016357421875\n",
      "model.layers.5.self_attn.o_proj 0.52734375 0.041015625\n",
      "model.layers.5.mlp.gate_proj 0.443359375 0.045654296875\n",
      "model.layers.5.mlp.up_proj 0.490234375 0.046630859375\n",
      "model.layers.5.mlp.down_proj 0.734375 0.05908203125\n",
      "model.layers.6.self_attn.q_proj 0.61328125 0.0311279296875\n",
      "model.layers.6.self_attn.k_proj 0.30859375 0.03173828125\n",
      "model.layers.6.self_attn.v_proj 0.287109375 0.031494140625\n",
      "model.layers.6.self_attn.o_proj 0.60546875 0.041259765625\n",
      "model.layers.6.mlp.gate_proj 0.41796875 0.044189453125\n",
      "model.layers.6.mlp.up_proj 0.56640625 0.045654296875\n",
      "model.layers.6.mlp.down_proj 0.921875 0.057373046875\n",
      "model.layers.7.self_attn.q_proj 0.7578125 0.029541015625\n",
      "model.layers.7.self_attn.k_proj 0.31640625 0.0303955078125\n",
      "model.layers.7.self_attn.v_proj 0.2255859375 0.0281982421875\n",
      "model.layers.7.self_attn.o_proj 0.55859375 0.043701171875\n",
      "model.layers.7.mlp.gate_proj 0.455078125 0.04736328125\n",
      "model.layers.7.mlp.up_proj 0.35546875 0.04345703125\n",
      "model.layers.7.mlp.down_proj 0.9453125 0.05322265625\n",
      "model.layers.8.self_attn.q_proj 0.55859375 0.0311279296875\n",
      "model.layers.8.self_attn.k_proj 0.30859375 0.031494140625\n",
      "model.layers.8.self_attn.v_proj 0.2080078125 0.03271484375\n",
      "model.layers.8.self_attn.o_proj 0.75390625 0.034912109375\n",
      "model.layers.8.mlp.gate_proj 0.390625 0.042236328125\n",
      "model.layers.8.mlp.up_proj 0.6484375 0.03662109375\n",
      "model.layers.8.mlp.down_proj 0.796875 0.0595703125\n",
      "model.layers.9.self_attn.q_proj 0.77734375 0.037109375\n",
      "model.layers.9.self_attn.k_proj 0.35546875 0.0361328125\n",
      "model.layers.9.self_attn.v_proj 0.2470703125 0.036865234375\n",
      "model.layers.9.self_attn.o_proj 0.546875 0.037353515625\n",
      "model.layers.9.mlp.gate_proj 0.375 0.041259765625\n",
      "model.layers.9.mlp.up_proj 0.609375 0.047607421875\n",
      "model.layers.9.mlp.down_proj 0.6640625 0.05908203125\n",
      "model.layers.10.self_attn.q_proj 0.462890625 0.03173828125\n",
      "model.layers.10.self_attn.k_proj 0.29296875 0.03369140625\n",
      "model.layers.10.self_attn.v_proj 0.259765625 0.0380859375\n",
      "model.layers.10.self_attn.o_proj 0.4140625 0.032958984375\n",
      "model.layers.10.mlp.gate_proj 0.52734375 0.035888671875\n",
      "model.layers.10.mlp.up_proj 0.392578125 0.044921875\n",
      "model.layers.10.mlp.down_proj 1.0234375 0.058349609375\n",
      "model.layers.11.self_attn.q_proj 0.609375 0.0286865234375\n",
      "model.layers.11.self_attn.k_proj 0.279296875 0.029541015625\n",
      "model.layers.11.self_attn.v_proj 0.298828125 0.03564453125\n",
      "model.layers.11.self_attn.o_proj 0.578125 0.038818359375\n",
      "model.layers.11.mlp.gate_proj 0.40234375 0.040283203125\n",
      "model.layers.11.mlp.up_proj 0.423828125 0.047607421875\n",
      "model.layers.11.mlp.down_proj 0.765625 0.0615234375\n",
      "model.layers.12.self_attn.q_proj 0.5234375 0.0311279296875\n",
      "model.layers.12.self_attn.k_proj 0.298828125 0.0303955078125\n",
      "model.layers.12.self_attn.v_proj 0.251953125 0.03857421875\n",
      "model.layers.12.self_attn.o_proj 0.51953125 0.044189453125\n",
      "model.layers.12.mlp.gate_proj 0.35546875 0.04443359375\n",
      "model.layers.12.mlp.up_proj 0.24609375 0.04833984375\n",
      "model.layers.12.mlp.down_proj 0.6484375 0.060791015625\n",
      "model.layers.13.self_attn.q_proj 0.6796875 0.0299072265625\n",
      "model.layers.13.self_attn.k_proj 0.30078125 0.027587890625\n",
      "model.layers.13.self_attn.v_proj 0.2294921875 0.039306640625\n",
      "model.layers.13.self_attn.o_proj 0.5078125 0.0439453125\n",
      "model.layers.13.mlp.gate_proj 0.3671875 0.03759765625\n",
      "model.layers.13.mlp.up_proj 0.431640625 0.048828125\n",
      "model.layers.13.mlp.down_proj 1.0703125 0.060791015625\n",
      "model.layers.14.self_attn.q_proj 0.5703125 0.0299072265625\n",
      "model.layers.14.self_attn.k_proj 0.314453125 0.0277099609375\n",
      "model.layers.14.self_attn.v_proj 0.333984375 0.03955078125\n",
      "model.layers.14.self_attn.o_proj 0.59765625 0.037841796875\n",
      "model.layers.14.mlp.gate_proj 0.341796875 0.04443359375\n",
      "model.layers.14.mlp.up_proj 0.4765625 0.05078125\n",
      "model.layers.14.mlp.down_proj 0.66015625 0.05908203125\n",
      "model.layers.15.self_attn.q_proj 0.58203125 0.027587890625\n",
      "model.layers.15.self_attn.k_proj 0.259765625 0.0289306640625\n",
      "model.layers.15.self_attn.v_proj 0.2265625 0.035400390625\n",
      "model.layers.15.self_attn.o_proj 0.48828125 0.048095703125\n",
      "model.layers.15.mlp.gate_proj 0.431640625 0.033447265625\n",
      "model.layers.15.mlp.up_proj 0.44140625 0.0478515625\n",
      "model.layers.15.mlp.down_proj 1.0625 0.060791015625\n",
      "model.layers.16.self_attn.q_proj 1.0 0.02587890625\n",
      "model.layers.16.self_attn.k_proj 0.2890625 0.02783203125\n",
      "model.layers.16.self_attn.v_proj 0.2421875 0.045654296875\n",
      "model.layers.16.self_attn.o_proj 0.42578125 0.050048828125\n",
      "model.layers.16.mlp.gate_proj 0.474609375 0.048095703125\n",
      "model.layers.16.mlp.up_proj 0.462890625 0.049560546875\n",
      "model.layers.16.mlp.down_proj 0.76953125 0.0615234375\n",
      "model.layers.17.self_attn.q_proj 0.9453125 0.0341796875\n",
      "model.layers.17.self_attn.k_proj 0.306640625 0.0291748046875\n",
      "model.layers.17.self_attn.v_proj 0.2138671875 0.042724609375\n",
      "model.layers.17.self_attn.o_proj 0.498046875 0.0498046875\n",
      "model.layers.17.mlp.gate_proj 0.404296875 0.051025390625\n",
      "model.layers.17.mlp.up_proj 0.33984375 0.05078125\n",
      "model.layers.17.mlp.down_proj 1.1484375 0.06103515625\n",
      "model.layers.18.self_attn.q_proj 0.7734375 0.027099609375\n",
      "model.layers.18.self_attn.k_proj 0.3125 0.0264892578125\n",
      "model.layers.18.self_attn.v_proj 0.224609375 0.046142578125\n",
      "model.layers.18.self_attn.o_proj 0.56640625 0.052001953125\n",
      "model.layers.18.mlp.gate_proj 0.369140625 0.052001953125\n",
      "model.layers.18.mlp.up_proj 0.326171875 0.052490234375\n",
      "model.layers.18.mlp.down_proj 1.1015625 0.0625\n",
      "model.layers.19.self_attn.q_proj 0.71875 0.02490234375\n",
      "model.layers.19.self_attn.k_proj 0.298828125 0.0264892578125\n",
      "model.layers.19.self_attn.v_proj 0.2236328125 0.044677734375\n",
      "model.layers.19.self_attn.o_proj 0.67578125 0.05322265625\n",
      "model.layers.19.mlp.gate_proj 0.38671875 0.0537109375\n",
      "model.layers.19.mlp.up_proj 0.5703125 0.052734375\n",
      "model.layers.19.mlp.down_proj 0.96875 0.060791015625\n",
      "model.layers.20.self_attn.q_proj 1.0546875 0.0283203125\n",
      "model.layers.20.self_attn.k_proj 0.328125 0.028076171875\n",
      "model.layers.20.self_attn.v_proj 0.1630859375 0.041748046875\n",
      "model.layers.20.self_attn.o_proj 0.578125 0.054443359375\n",
      "model.layers.20.mlp.gate_proj 0.322265625 0.05126953125\n",
      "model.layers.20.mlp.up_proj 0.318359375 0.052001953125\n",
      "model.layers.20.mlp.down_proj 1.109375 0.06201171875\n",
      "model.layers.21.self_attn.q_proj 0.94140625 0.0250244140625\n",
      "model.layers.21.self_attn.k_proj 0.375 0.024658203125\n",
      "model.layers.21.self_attn.v_proj 0.1796875 0.05029296875\n",
      "model.layers.21.self_attn.o_proj 0.55078125 0.053466796875\n",
      "model.layers.21.mlp.gate_proj 0.296875 0.0537109375\n",
      "model.layers.21.mlp.up_proj 0.2333984375 0.052001953125\n",
      "model.layers.21.mlp.down_proj 0.734375 0.060546875\n",
      "model.layers.22.self_attn.q_proj 0.875 0.0228271484375\n",
      "model.layers.22.self_attn.k_proj 0.31640625 0.0240478515625\n",
      "model.layers.22.self_attn.v_proj 0.154296875 0.04833984375\n",
      "model.layers.22.self_attn.o_proj 0.7734375 0.055419921875\n",
      "model.layers.22.mlp.gate_proj 0.37109375 0.054443359375\n",
      "model.layers.22.mlp.up_proj 0.275390625 0.052734375\n",
      "model.layers.22.mlp.down_proj 0.388671875 0.061767578125\n",
      "model.layers.23.self_attn.q_proj 0.81640625 0.025146484375\n",
      "model.layers.23.self_attn.k_proj 0.3125 0.0238037109375\n",
      "model.layers.23.self_attn.v_proj 0.1533203125 0.04443359375\n",
      "model.layers.23.self_attn.o_proj 0.484375 0.053955078125\n",
      "model.layers.23.mlp.gate_proj 0.416015625 0.05517578125\n",
      "model.layers.23.mlp.up_proj 0.296875 0.053955078125\n",
      "model.layers.23.mlp.down_proj 1.3125 0.061767578125\n",
      "model.layers.24.self_attn.q_proj 1.03125 0.02490234375\n",
      "model.layers.24.self_attn.k_proj 0.330078125 0.0234375\n",
      "model.layers.24.self_attn.v_proj 0.1845703125 0.052978515625\n",
      "model.layers.24.self_attn.o_proj 0.73046875 0.058349609375\n",
      "model.layers.24.mlp.gate_proj 0.34765625 0.056884765625\n",
      "model.layers.24.mlp.up_proj 0.1982421875 0.05224609375\n",
      "model.layers.24.mlp.down_proj 0.7265625 0.056884765625\n",
      "model.layers.25.self_attn.q_proj 0.64453125 0.0247802734375\n",
      "model.layers.25.self_attn.k_proj 0.365234375 0.025146484375\n",
      "model.layers.25.self_attn.v_proj 0.1259765625 0.05126953125\n",
      "model.layers.25.self_attn.o_proj 0.65234375 0.060302734375\n",
      "model.layers.25.mlp.gate_proj 0.5546875 0.05712890625\n",
      "model.layers.25.mlp.up_proj 0.251953125 0.052734375\n",
      "model.layers.25.mlp.down_proj 1.03125 0.052734375\n",
      "model.layers.26.self_attn.q_proj 0.86328125 0.0242919921875\n",
      "model.layers.26.self_attn.k_proj 0.35546875 0.022216796875\n",
      "model.layers.26.self_attn.v_proj 0.1962890625 0.03369140625\n",
      "model.layers.26.self_attn.o_proj 0.6875 0.060791015625\n",
      "model.layers.26.mlp.gate_proj 0.408203125 0.056640625\n",
      "model.layers.26.mlp.up_proj 0.3671875 0.05078125\n",
      "model.layers.26.mlp.down_proj 0.578125 0.061279296875\n",
      "model.layers.27.self_attn.q_proj 0.79296875 0.0380859375\n",
      "model.layers.27.self_attn.k_proj 0.2890625 0.037841796875\n",
      "model.layers.27.self_attn.v_proj 0.1328125 0.050537109375\n",
      "model.layers.27.self_attn.o_proj 0.6328125 0.057861328125\n",
      "model.layers.27.mlp.gate_proj 0.6328125 0.056396484375\n",
      "model.layers.27.mlp.up_proj 0.345703125 0.0498046875\n",
      "model.layers.27.mlp.down_proj 0.76171875 0.06201171875\n",
      "model.layers.28.self_attn.q_proj 0.76953125 0.0238037109375\n",
      "model.layers.28.self_attn.k_proj 0.369140625 0.024658203125\n",
      "model.layers.28.self_attn.v_proj 0.1669921875 0.04736328125\n",
      "model.layers.28.self_attn.o_proj 0.78125 0.05908203125\n",
      "model.layers.28.mlp.gate_proj 0.3984375 0.054931640625\n",
      "model.layers.28.mlp.up_proj 0.376953125 0.052734375\n",
      "model.layers.28.mlp.down_proj 1.0 0.064453125\n",
      "model.layers.29.self_attn.q_proj 0.81640625 0.024658203125\n",
      "model.layers.29.self_attn.k_proj 0.3671875 0.023681640625\n",
      "model.layers.29.self_attn.v_proj 0.130859375 0.048095703125\n",
      "model.layers.29.self_attn.o_proj 0.6328125 0.059814453125\n",
      "model.layers.29.mlp.gate_proj 0.46484375 0.054443359375\n",
      "model.layers.29.mlp.up_proj 0.546875 0.04736328125\n",
      "model.layers.29.mlp.down_proj 0.984375 0.06201171875\n",
      "model.layers.30.self_attn.q_proj 0.81640625 0.0341796875\n",
      "model.layers.30.self_attn.k_proj 0.369140625 0.034423828125\n",
      "model.layers.30.self_attn.v_proj 0.5 0.051513671875\n",
      "model.layers.30.self_attn.o_proj 0.625 0.064453125\n",
      "model.layers.30.mlp.gate_proj 0.796875 0.052734375\n",
      "model.layers.30.mlp.up_proj 0.77734375 0.05078125\n",
      "model.layers.30.mlp.down_proj 1.3046875 0.06201171875\n",
      "model.layers.31.self_attn.q_proj 0.66796875 0.0308837890625\n",
      "model.layers.31.self_attn.k_proj 0.404296875 0.028564453125\n",
      "model.layers.31.self_attn.v_proj 0.390625 0.041015625\n",
      "model.layers.31.self_attn.o_proj 1.3125 0.0595703125\n",
      "model.layers.31.mlp.gate_proj 0.380859375 0.051025390625\n",
      "model.layers.31.mlp.up_proj 0.75390625 0.048583984375\n",
      "model.layers.31.mlp.down_proj 1.7734375 0.064453125\n",
      "lm_head 0.34765625 0.034912109375\n"
     ]
    }
   ],
   "source": [
    "for key in act_scales.keys():\n",
    "    # print(key)\n",
    "    # print(smooth_scales[key])\n",
    "    # print(act_scales[key]['input'].max().item(), act_scales[key]['input'].min().item())\n",
    "    # print(act_w_scales[key]['input'].max().item(), act_scales[key]['input'].min().item())\n",
    "    # print(smooth_act_scales[key]['input'])\n",
    "    # print(key, act_scales[key]['input'].max().item(), act_scales[key]['output'].max().item())\n",
    "    print(key, act_w_scales[key]['weight'].max().item(), act_w_scales[key]['weight'].min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer, q_input, q_output, k_input, k_output, v_input, v_output, o_input, o_output, gate_input, gate_output, up_input, up_output, down_input, down_output\n",
      "Layer 1, 4.97265625, 12.65625, 4.97265625, 6.12109375, 4.97265625, 1.2568359375, 1.2568359375, 0.93701171875, 2.275390625, 3.50390625, 2.275390625, 1.7900390625, 2.435546875, 2.052734375\n",
      "Layer 2, 9.5390625, 11.015625, 9.5390625, 8.546875, 9.5390625, 1.806640625, 1.2919921875, 2.318359375, 1.490234375, 44.0, 1.490234375, 38.0625, 1675.0, 2718.0\n",
      "Layer 3, 8.765625, 11.234375, 8.765625, 14.0859375, 8.765625, 3.646484375, 1.2890625, 3.099609375, 5.6875, 5.015625, 5.6875, 2.880859375, 4.625, 4.296875\n",
      "Layer 4, 10.03125, 14.0859375, 10.03125, 16.453125, 10.03125, 3.66796875, 2.751953125, 4.37890625, 3.4765625, 6.984375, 3.4765625, 3.20703125, 10.9765625, 6.6875\n",
      "Layer 5, 10.828125, 12.40625, 10.828125, 18.59375, 10.828125, 4.16015625, 2.87890625, 3.4296875, 7.40234375, 5.51171875, 7.40234375, 3.78125, 11.5390625, 9.421875\n",
      "Layer 6, 10.9609375, 12.6796875, 10.9609375, 19.984375, 10.9609375, 4.71484375, 2.822265625, 4.68359375, 5.3203125, 5.31640625, 5.3203125, 7.21875, 10.1875, 5.53125\n",
      "Layer 7, 13.1328125, 17.765625, 13.1328125, 19.71875, 13.1328125, 4.703125, 4.05078125, 4.91015625, 5.98828125, 6.796875, 5.98828125, 4.26171875, 15.1484375, 10.75\n",
      "Layer 8, 13.5625, 22.359375, 13.5625, 20.015625, 13.5625, 5.27734375, 3.98046875, 4.8125, 6.8984375, 12.7421875, 6.8984375, 6.8359375, 16.984375, 14.2734375\n",
      "Layer 9, 19.765625, 17.484375, 19.765625, 19.046875, 19.765625, 5.49609375, 2.828125, 6.66796875, 9.0703125, 15.7890625, 9.0703125, 10.5234375, 37.125, 26.015625\n",
      "Layer 10, 24.8125, 13.96875, 24.8125, 19.625, 24.8125, 7.03515625, 3.384765625, 5.33203125, 11.5625, 9.0546875, 11.5625, 7.70703125, 14.625, 10.3203125\n",
      "Layer 11, 33.46875, 14.765625, 33.46875, 19.3125, 33.46875, 9.2734375, 4.2421875, 6.51953125, 6.46484375, 8.25, 6.46484375, 5.80859375, 8.9375, 7.41796875\n",
      "Layer 12, 27.296875, 22.984375, 27.296875, 18.5, 27.296875, 8.53125, 3.50390625, 5.74609375, 6.6484375, 10.234375, 6.6484375, 5.44140625, 10.546875, 5.2421875\n",
      "Layer 13, 28.09375, 15.9921875, 28.09375, 18.25, 28.09375, 7.4765625, 3.01171875, 5.0859375, 8.1484375, 7.6640625, 8.1484375, 5.21875, 10.953125, 4.61328125\n",
      "Layer 14, 23.640625, 15.71875, 23.640625, 19.671875, 23.640625, 7.66796875, 4.8828125, 5.7109375, 9.890625, 8.5390625, 9.890625, 5.484375, 12.3046875, 6.9375\n",
      "Layer 15, 24.515625, 19.640625, 24.515625, 18.796875, 24.515625, 8.8671875, 3.365234375, 6.4375, 8.6328125, 9.6640625, 8.6328125, 6.55078125, 15.4296875, 5.453125\n",
      "Layer 16, 23.0, 16.25, 23.0, 19.484375, 23.0, 6.8828125, 3.58203125, 6.02734375, 9.9453125, 9.859375, 9.9453125, 7.09765625, 14.0234375, 7.28515625\n",
      "Layer 17, 22.203125, 20.859375, 22.203125, 18.828125, 22.203125, 6.078125, 3.4296875, 7.16796875, 9.796875, 9.7421875, 9.796875, 7.59375, 27.046875, 7.9765625\n",
      "Layer 18, 17.90625, 18.203125, 17.90625, 24.09375, 17.90625, 4.8125, 3.876953125, 6.796875, 9.1875, 10.09375, 9.1875, 8.953125, 22.625, 7.0859375\n",
      "Layer 19, 20.015625, 17.28125, 20.015625, 20.03125, 20.015625, 5.953125, 5.3359375, 5.1328125, 8.9375, 10.546875, 8.9375, 7.32421875, 36.71875, 10.0234375\n",
      "Layer 20, 19.515625, 17.40625, 19.515625, 19.59375, 19.515625, 5.3515625, 4.17578125, 7.265625, 9.203125, 11.2421875, 9.203125, 9.6953125, 41.53125, 10.2578125\n",
      "Layer 21, 19.96875, 17.765625, 19.96875, 19.53125, 19.96875, 7.0390625, 5.46875, 14.796875, 8.40625, 12.375, 8.40625, 9.453125, 36.1875, 9.1015625\n",
      "Layer 22, 19.546875, 19.265625, 19.546875, 19.0625, 19.546875, 6.5, 5.828125, 12.2109375, 8.859375, 11.9375, 8.859375, 10.2734375, 34.96875, 7.07421875\n",
      "Layer 23, 20.25, 19.5, 20.25, 20.703125, 20.25, 8.234375, 8.015625, 13.96875, 8.6796875, 14.59375, 8.6796875, 9.9453125, 43.65625, 6.63671875\n",
      "Layer 24, 19.671875, 16.671875, 19.671875, 19.0625, 19.671875, 7.046875, 6.3671875, 18.484375, 8.2109375, 12.15625, 8.2109375, 9.015625, 39.8125, 8.0390625\n",
      "Layer 25, 22.03125, 17.484375, 22.03125, 20.40625, 22.03125, 8.6796875, 8.6796875, 21.40625, 9.1171875, 13.4453125, 9.1171875, 8.375, 44.75, 6.765625\n",
      "Layer 26, 19.390625, 15.4140625, 19.390625, 20.265625, 19.390625, 7.88671875, 5.72265625, 4.9609375, 9.1015625, 15.484375, 9.1015625, 11.015625, 53.84375, 17.828125\n",
      "Layer 27, 21.296875, 15.5390625, 21.296875, 20.90625, 21.296875, 8.7578125, 6.88671875, 23.671875, 10.09375, 12.7734375, 10.09375, 9.2734375, 49.84375, 7.60546875\n",
      "Layer 28, 14.9609375, 15.25, 14.9609375, 21.0, 14.9609375, 9.5390625, 7.3671875, 12.7578125, 11.0546875, 14.6484375, 11.0546875, 15.9921875, 63.90625, 9.046875\n",
      "Layer 29, 15.015625, 17.671875, 15.015625, 20.375, 15.015625, 8.0, 6.53125, 11.1328125, 12.0234375, 20.765625, 12.0234375, 13.71875, 62.59375, 13.0\n",
      "Layer 30, 17.21875, 18.203125, 17.21875, 21.4375, 17.21875, 10.7734375, 6.6875, 24.015625, 12.703125, 19.21875, 12.703125, 20.625, 75.1875, 46.4375\n",
      "Layer 31, 13.1796875, 18.109375, 13.1796875, 20.265625, 13.1796875, 11.3984375, 9.3984375, 18.828125, 16.875, 77.3125, 16.875, 67.3125, 1267.0, 2522.0\n",
      "Layer 32, 14.8515625, 21.6875, 14.8515625, 25.140625, 14.8515625, 13.546875, 11.3203125, 236.875, 18.421875, 35.1875, 18.421875, 50.9375, 273.75, 614.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'layer_act_scales.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projections = [\"q\", \"k\", \"v\", \"o\"]\n",
    "mlps = [\"gate\", \"up\", \"down\"]\n",
    "header_projections = [f\"{proj}_input, {proj}_output\" for proj in projections]\n",
    "header_mlps = [f\"{mlp}_input, {mlp}_output\" for mlp in mlps]\n",
    "csv_header = f\"Layer, \" + \", \".join(header_projections) + \", \" + \", \".join(header_mlps) + \"\\n\"\n",
    "csv_lines = [csv_header]\n",
    "\n",
    "# 收集每層的數據\n",
    "for i in range(32):  # 假設有 32 層\n",
    "    layer_data = [f\"Layer {i+1}\"]\n",
    "    for proj in projections:\n",
    "        input_key = f\"model.layers.{i}.self_attn.{proj}_proj\"\n",
    "        output_key = f\"model.layers.{i}.self_attn.{proj}_proj\"\n",
    "        input_max = act_scales.get(input_key, {}).get(\"input\", torch.tensor([float('nan')])).max().item()\n",
    "        output_max = act_scales.get(output_key, {}).get(\"output\", torch.tensor([float('nan')])).max().item()\n",
    "        layer_data.append(f\"{input_max}\")\n",
    "        layer_data.append(f\"{output_max}\")\n",
    "    for mlp in mlps:\n",
    "        input_key = f\"model.layers.{i}.mlp.{mlp}_proj\"\n",
    "        output_key = f\"model.layers.{i}.mlp.{mlp}_proj\"\n",
    "        input_max = act_scales.get(input_key, {}).get(\"input\", torch.tensor([float('nan')])).max().item()\n",
    "        output_max = act_scales.get(output_key, {}).get(\"output\", torch.tensor([float('nan')])).max().item()\n",
    "        layer_data.append(f\"{input_max}\")\n",
    "        layer_data.append(f\"{output_max}\")\n",
    "    csv_lines.append(\", \".join(layer_data) + \"\\n\")\n",
    "\n",
    "# 將 CSV 內容合併成一個字符串\n",
    "csv_content = \"\".join(csv_lines)\n",
    "\n",
    "# 打印 CSV 內容（或保存到文件）\n",
    "print(csv_content)\n",
    "\n",
    "# 如果需要將結果保存到文件\n",
    "file_path = 'layer_act_scales.csv'\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(csv_content)\n",
    "\n",
    "file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
